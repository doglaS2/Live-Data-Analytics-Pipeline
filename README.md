# RealTime Data Pipeline com Kafka, Spark e Python

Este projeto implementa um pipeline de dados em tempo real que gera, ingere, processa e visualiza eventos contÃ­nuos utilizando Docker para orquestraÃ§Ã£o. Ele integra Kafka, Spark e uma aplicaÃ§Ã£o Python para processar dados em streaming.

## Arquitetura do Sistema

```
Python App -> Gera Eventos
     â†“
   Kafka   -> Armazena Eventos
     â†“
   Spark   -> Processa Eventos
```

### Componentes

1. **Python App**
   - Gera eventos aleatÃ³rios (ID, valor e timestamp)
   - Envia eventos para o Kafka
   - Utiliza a biblioteca `kafka-python`

2. **Kafka**
   - Sistema de mensageria distribuÃ­do
   - Armazena os eventos em tÃ³picos
   - Permite mÃºltiplos consumidores

3. **Spark**
   - Processa os dados em tempo real
   - Realiza transformaÃ§Ãµes nos eventos
   - EscalÃ¡vel para grandes volumes de dados

## Tecnologias Utilizadas

### Por que Python?
- Linguagem fÃ¡cil de usar
- Boas bibliotecas para Kafka
- Ideal para scripts e automaÃ§Ã£o

### Por que Java?
- NecessÃ¡rio para Spark e Kafka
- Spark Ã© escrito em Java
- Kafka Ã© escrito em Java
- Muitas ferramentas de Big Data usam Java como base

### Por que Debian?
- Base da imagem Python oficial
- Mais leve que Ubuntu
- Mais estÃ¡vel
- Usamos a imagem `python:3.9-slim`

### Por que Docker?
- Isola cada componente
- Garante versÃµes consistentes
- FÃ¡cil de executar em qualquer mÃ¡quina
- Simplifica a configuraÃ§Ã£o do ambiente

## Estrutura do Projeto

```
docker-kafka-spark-python
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ kafka/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ python-app/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Tecnologias Utilizadas

- ğŸ Python (geraÃ§Ã£o e envio de eventos em JSON)
- ğŸ§­ Apache Kafka (mensageria para eventos em tempo real)
- âš¡ Apache Spark (processamento de dados em streaming)
- ğŸ³ Docker / Docker Compose (containerizaÃ§Ã£o e orquestraÃ§Ã£o)

## Requisitos

- Docker instalado na mÃ¡quina  
- Docker Compose instalado
- 4GB de RAM (mÃ­nimo)
- 10GB de espaÃ§o em disco

## Como Rodar o Projeto

1. Clone ou faÃ§a download do repositÃ³rio.

2. Navegue atÃ© o diretÃ³rio do projeto:

   ```bash
   cd docker-kafka-spark-python
   ```

3. Execute para construir e subir os containers:

   ```bash
   docker-compose up --build
   ```

## Como Usar

- A aplicaÃ§Ã£o Python envia eventos JSON para o Kafka
- Spark consome e processa esses eventos em tempo real
- Monitore os logs dos containers para acompanhar a atividade de cada serviÃ§o

## Parar os ServiÃ§os

Para interromper e remover os containers:

```bash
docker-compose down
```

## Notas Importantes

### DependÃªncias
- O Spark requer Java 8 para funcionar corretamente
- As dependÃªncias sÃ£o baixadas automaticamente na primeira execuÃ§Ã£o
- O download das dependÃªncias pode levar alguns minutos

### Logs
- "Streaming query has been idle" Ã© normal - significa que o Spark estÃ¡ esperando novos dados
- Os eventos sÃ£o processados em lotes (batches)
- O Spark mostra estatÃ­sticas de processamento

## SoluÃ§Ã£o de Problemas

### Problemas Comuns

1. **Erro de Java**
   - Verifique se estÃ¡ usando Java 8
   - Confira o `JAVA_HOME` no Dockerfile

2. **Kafka nÃ£o inicia**
   - Verifique se o Zookeeper estÃ¡ rodando
   - Confira as portas no `docker-compose.yml`

3. **Spark nÃ£o processa dados**
   - Verifique se o Kafka estÃ¡ recebendo eventos
   - Confira os logs do Spark para erros

## PersonalizaÃ§Ã£o

- Modifique `python-app/app.py` para implementar a lÃ³gica da sua aplicaÃ§Ã£o
- Atualize `python-app/requirements.txt` para incluir dependÃªncias Python adicionais

## VisÃ£o Geral

Este pipeline exemplifica um fluxo tÃ­pico de dados em tempo real com geraÃ§Ã£o, ingestÃ£o, processamento e anÃ¡lise utilizando ferramentas populares. Pode ser expandido para incluir armazenamento analÃ­tico (como Apache Druid) e visualizaÃ§Ã£o (como Metabase).

## PrÃ³ximos Passos

O pipeline pode ser estendido para:
1. Adicionar mais transformaÃ§Ãµes nos dados
2. Calcular estatÃ­sticas (mÃ©dia, mÃ¡ximo, mÃ­nimo)
3. Filtrar eventos especÃ­ficos
4. Agrupar eventos por critÃ©rios
5. Adicionar visualizaÃ§Ã£o dos dados

## Contribuindo

1. FaÃ§a um fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/nova-feature`)
3. Commit suas mudanÃ§as (`git commit -am 'Adiciona nova feature'`)
4. Push para a branch (`git push origin feature/nova-feature`)
5. Crie um Pull Request

## LicenÃ§a

Este projeto estÃ¡ disponÃ­vel sob a licenÃ§a MIT.