# RealTime Data Pipeline com Kafka, Spark e Python

Este projeto implementa um pipeline de dados em tempo real que gera, ingere, processa e visualiza eventos contÃ­nuos utilizando Docker para orquestraÃ§Ã£o. Ele integra Kafka, Spark e uma aplicaÃ§Ã£o Python para processar dados em streaming.

---

## Estrutura do Projeto

docker-kafka-spark-python
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ kafka/
â”‚ â””â”€â”€ Dockerfile
â”œâ”€â”€ spark/
â”‚ â””â”€â”€ Dockerfile
â”œâ”€â”€ python-app/
â”‚ â”œâ”€â”€ Dockerfile
â”‚ â”œâ”€â”€ app.py
â”‚ â””â”€â”€ requirements.txt
â””â”€â”€ README.md


---

## Tecnologias Utilizadas

- ğŸ Python (geraÃ§Ã£o e envio de eventos em JSON)
- ğŸ§­ Apache Kafka (mensageria para eventos em tempo real)
- âš¡ Apache Spark (processamento de dados em streaming)
- ğŸ³ Docker / Docker Compose (containerizaÃ§Ã£o e orquestraÃ§Ã£o)

---

## Requisitos

- Docker instalado na mÃ¡quina  
- Docker Compose instalado

---

## Como Rodar o Projeto

1. Clone ou faÃ§a download do repositÃ³rio.

2. Navegue atÃ© o diretÃ³rio do projeto:

   ```bash
   cd docker-kafka-spark-python
Execute para construir e subir os containers:

bash
Copiar
Editar
docker-compose up --build
Como Usar
A aplicaÃ§Ã£o Python envia eventos JSON para o Kafka.

Spark consome e processa esses eventos em tempo real.

Monitore os logs dos containers para acompanhar a atividade de cada serviÃ§o.

Parar os ServiÃ§os
Para interromper e remover os containers:

bash
Copiar
Editar
docker-compose down
PersonalizaÃ§Ã£o
Modifique python-app/app.py para implementar a lÃ³gica da sua aplicaÃ§Ã£o.

Atualize python-app/requirements.txt para incluir dependÃªncias Python adicionais.

VisÃ£o Geral
Este pipeline exemplifica um fluxo tÃ­pico de dados em tempo real com geraÃ§Ã£o, ingestÃ£o, processamento e anÃ¡lise utilizando ferramentas populares. Pode ser expandido para incluir armazenamento analÃ­tico (como Apache Druid) e visualizaÃ§Ã£o (como Metabase).

LicenÃ§a
Este projeto estÃ¡ disponÃ­vel sob a licenÃ§a MIT (ou outra que desejar).
