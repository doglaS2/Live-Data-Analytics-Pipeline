# RealTime Data Pipeline com Kafka, Spark e Python

Este projeto implementa um pipeline de dados em tempo real que gera, ingere, processa e visualiza eventos contÃ­nuos utilizando Docker para orquestraÃ§Ã£o. Ele integra Kafka, Spark, Druid e Metabase para processar e visualizar dados em streaming.

## Arquitetura do Sistema

```
Python App -> Gera Eventos
     â†“
   Kafka   -> Armazena Eventos
     â†“
   Druid   -> Processa e Armazena
     â†“
   Spark   -> Processa Eventos
     â†“
  Metabase -> Visualiza Dados
```

### Componentes

1. **Python App**
   - Gera eventos aleatÃ³rios (ID, valor e timestamp)
   - Envia eventos para o Kafka
   - Utiliza a biblioteca `kafka-python`

2. **Kafka**
   - Sistema de mensageria distribuÃ­do
   - Armazena os eventos em tÃ³picos
   - Permite mÃºltiplos consumidores

3. **Druid**
   - Processa e armazena dados em tempo real
   - Permite consultas SQL em dados de streaming
   - Componentes:
     - Coordinator (porta 8081)
     - Broker (porta 8082)
     - MiddleManager (porta 8091)

4. **Spark**
   - Processa os dados em tempo real
   - Realiza transformaÃ§Ãµes nos eventos
   - EscalÃ¡vel para grandes volumes de dados

5. **Metabase**
   - VisualizaÃ§Ã£o de dados em tempo real
   - Dashboards interativos
   - GrÃ¡ficos e mÃ©tricas

## Tecnologias Utilizadas

- ğŸ Python (geraÃ§Ã£o e envio de eventos em JSON)
- ğŸ§­ Apache Kafka (mensageria para eventos em tempo real)
- âš¡ Apache Druid (processamento e armazenamento de dados em tempo real)
- ğŸ”¥ Apache Spark (processamento de dados em streaming)
- ğŸ“Š Metabase (visualizaÃ§Ã£o e dashboards)
- ğŸ³ Docker / Docker Compose (containerizaÃ§Ã£o e orquestraÃ§Ã£o)

## Requisitos

- Docker instalado na mÃ¡quina  
- Docker Compose instalado
- 8GB de RAM (mÃ­nimo)
- 20GB de espaÃ§o em disco
- Processador com 4 cores ou mais
- Sistema operacional Linux recomendado para melhor performance

## Estrutura do Projeto

```
docker-kafka-spark-python
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ kafka/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ python-app/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Tecnologias Utilizadas

- ğŸ Python (geraÃ§Ã£o e envio de eventos em JSON)
- ğŸ§­ Apache Kafka (mensageria para eventos em tempo real)
- âš¡ Apache Druid (processamento e armazenamento de dados em tempo real)
- ğŸ”¥ Apache Spark (processamento de dados em streaming)
- ğŸ“Š Metabase (visualizaÃ§Ã£o e dashboards)
- ğŸ³ Docker / Docker Compose (containerizaÃ§Ã£o e orquestraÃ§Ã£o)

## Requisitos

- Docker instalado na mÃ¡quina  
- Docker Compose instalado
- 8GB de RAM (mÃ­nimo)
- 20GB de espaÃ§o em disco
- Processador com 4 cores ou mais
- Sistema operacional Linux recomendado para melhor performance

## Como Rodar o Projeto

1. Clone ou faÃ§a download do repositÃ³rio.

2. Navegue atÃ© o diretÃ³rio do projeto:

   ```bash
   cd docker-kafka-spark-python
   ```

3. Execute para construir e subir os containers:

   ```bash
   docker-compose up --build
   ```

## Como Usar

- A aplicaÃ§Ã£o Python envia eventos JSON para o Kafka
- Druid ingere e processa os dados em tempo real
- Spark consome e processa esses eventos
- Metabase visualiza os dados processados
- Monitore os logs dos containers para acompanhar a atividade de cada serviÃ§o

### Acessando os ServiÃ§os

- **Druid Coordinator**: http://localhost:8081
  - Acesse "Query" para executar consultas SQL
  - Acesse "Streams" para verificar ingestÃ£o de dados

- **Metabase**: http://localhost:3000
  - Primeiro acesso: configure o banco de dados (PostgreSQL)
  - Host: metabase-db
  - Porta: 5432
  - Database: metabase
  - UsuÃ¡rio: metabase
  - Senha: metabase

### Exemplo de Consulta no Druid

```sql
SELECT *
FROM "events"
LIMIT 10
```

### Exemplo de Dashboard no Metabase

1. Crie uma nova pergunta
2. Selecione "Custom SQL"
3. Use a query:
```sql
SELECT 
  DATE_TRUNC('hour', timestamp) as hora,
  COUNT(*) as total_eventos,
  AVG(valor) as media_valor
FROM events
GROUP BY 1
ORDER BY 1
```

4. Visualize como grÃ¡fico de linha

## Parar os ServiÃ§os

Para interromper e remover os containers:

```bash
docker-compose down
```

## Notas Importantes

### DependÃªncias
- O Spark requer Java 8 para funcionar corretamente
- As dependÃªncias sÃ£o baixadas automaticamente na primeira execuÃ§Ã£o
- O download das dependÃªncias pode levar alguns minutos

### Logs
- "Streaming query has been idle" Ã© normal - significa que o Spark estÃ¡ esperando novos dados
- Os eventos sÃ£o processados em lotes (batches)
- O Spark mostra estatÃ­sticas de processamento

## SoluÃ§Ã£o de Problemas

### Problemas Comuns

1. **Erro de Java**
   - Verifique se estÃ¡ usando Java 8
   - Confira o `JAVA_HOME` no Dockerfile

2. **Kafka nÃ£o inicia**
   - Verifique se o Zookeeper estÃ¡ rodando
   - Confira as portas no `docker-compose.yml`

3. **Spark nÃ£o processa dados**
   - Verifique se o Kafka estÃ¡ recebendo eventos
   - Confira os logs do Spark para erros

4. **Druid nÃ£o mostra dados**
   - Verifique se o Kafka estÃ¡ saudÃ¡vel
   - Confirme se o tÃ³pico "events" existe
   - Verifique os logs do Druid

5. **Metabase sem conexÃ£o**
   - Verifique se o PostgreSQL estÃ¡ rodando
   - Confirme as credenciais de conexÃ£o
   - Verifique os logs do Metabase

## PersonalizaÃ§Ã£o

- Modifique `python-app/app.py` para implementar a lÃ³gica da sua aplicaÃ§Ã£o
- Atualize `python-app/requirements.txt` para incluir dependÃªncias Python adicionais

## VisÃ£o Geral

Este pipeline exemplifica um fluxo tÃ­pico de dados em tempo real com geraÃ§Ã£o, ingestÃ£o, processamento e anÃ¡lise utilizando ferramentas populares. Pode ser expandido para incluir armazenamento analÃ­tico (como Apache Druid) e visualizaÃ§Ã£o (como Metabase).

## PrÃ³ximos Passos

O pipeline pode ser estendido para:
1. Adicionar mais transformaÃ§Ãµes nos dados
2. Calcular estatÃ­sticas (mÃ©dia, mÃ¡ximo, mÃ­nimo)
3. Filtrar eventos especÃ­ficos
4. Agrupar eventos por critÃ©rios
5. Criar dashboards personalizados no Metabase
6. Adicionar mais visualizaÃ§Ãµes e mÃ©tricas

## Contribuindo

1. FaÃ§a um fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/nova-feature`)
3. Commit suas mudanÃ§as (`git commit -am 'Adiciona nova feature'`)
4. Push para a branch (`git push origin feature/nova-feature`)
5. Crie um Pull Request

## LicenÃ§a

Este projeto estÃ¡ disponÃ­vel sob a licenÃ§a MIT.